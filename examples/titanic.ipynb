{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/CreateTool-AGI/blob/main/examples/titanic.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/fuyu-quant/CreateTool-AGI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "from createtoolagi import CTAGI\n",
    "\n",
    "#import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "import pkg_resources\n",
    "package_name = \"createtoolagi\"  \n",
    "pkg = pkg_resources.get_distribution(package_name)\n",
    "print(pkg.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ../tools/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "training_data = ''\n",
    "with open('../data/learning.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        training_data += ','.join(row) + '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = \"2,male,27.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "{training_data}_\n",
    "------------------\n",
    "Answer between 0~1 what happened to the \"SURVIVED\" of the following people when the above data was available.\n",
    "Output should be numeric only.\n",
    "------------------\n",
    "pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "{predict_data_}\n",
    "\"\"\".format(training_data = training_data, predict_data_ = predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model_name = 'gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CreateTool-AGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CTAGI.__init__() got an unexpected keyword argument 'qdrant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m qdrant \u001b[38;5;241m=\u001b[39m QdrantClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../tools/titanic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m qdrant\u001b[38;5;241m.\u001b[39mcreate_collection(collection_name \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic\u001b[39m\u001b[38;5;124m\"\u001b[39m, vectors_config\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mVectorParams(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1536\u001b[39m, distance\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistance\u001b[38;5;241m.\u001b[39mCOSINE))\n\u001b[0;32m---> 14\u001b[0m ctagi \u001b[38;5;241m=\u001b[39m \u001b[43mCTAGI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43membegging_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membegging_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqdrant\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CTAGI.__init__() got an unexpected keyword argument 'qdrant'"
     ]
    }
   ],
   "source": [
    "from createtoolagi import CTAGI\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "\n",
    "base_model_name = 'gpt-4'\n",
    "#base_model_name = 'gpt-3.5-turbo'\n",
    "create_model_name = 'gpt-4'\n",
    "embegging_model_name = 'text-embedding-ada-002'\n",
    "\n",
    "qdrant = QdrantClient(path='../tools/titanic')\n",
    "qdrant.create_collection(collection_name =\"titanic\", vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE))\n",
    "\n",
    "ctagi = CTAGI(\n",
    "    base_model_name=base_model_name, \n",
    "    create_model_name=create_model_name, \n",
    "    embegging_model_name=embegging_model_name,\n",
    "    qdrant=qdrant\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ctagi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 111\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124msurvived,pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m0,2,male,52.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124m2,male,27.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mctagi\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ctagi' is not defined"
     ]
    }
   ],
   "source": [
    "input = \"\"\"\n",
    "survived,pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "0,2,male,52.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.25,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,18.0,0,0,7.775,S,Third,woman,False,,Southampton,no,True\n",
    "1,3,female,16.0,0,0,7.75,Q,Third,woman,False,,Queenstown,yes,True\n",
    "0,2,male,19.0,1,1,36.75,S,Second,man,True,,Southampton,no,False\n",
    "0,3,male,28.0,0,0,56.4958,S,Third,man,True,,Southampton,no,True\n",
    "1,1,female,53.0,2,0,51.4792,S,First,woman,False,C,Southampton,yes,False\n",
    "1,3,female,4.0,0,1,13.4167,C,Third,child,False,,Cherbourg,yes,False\n",
    "1,3,male,32.0,0,0,7.925,S,Third,man,True,,Southampton,yes,True\n",
    "0,2,male,23.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,1,female,25.0,1,2,151.55,S,First,woman,False,C,Southampton,no,False\n",
    "1,3,male,20.0,0,0,7.2292,C,Third,man,True,,Cherbourg,yes,True\n",
    "0,3,male,19.0,0,0,7.65,S,Third,man,True,F,Southampton,no,True\n",
    "0,3,male,39.0,0,0,24.15,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,32.0,0,0,56.4958,S,Third,man,True,,Southampton,yes,True\n",
    "0,2,male,39.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "1,1,female,31.0,0,2,164.8667,S,First,woman,False,C,Southampton,yes,False\n",
    "1,3,male,20.0,1,0,7.925,S,Third,man,True,,Southampton,yes,False\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,51.0,0,0,7.0542,S,Third,man,True,,Southampton,no,True\n",
    "1,3,female,18.0,0,0,9.8417,S,Third,woman,False,,Southampton,yes,True\n",
    "1,1,male,49.0,1,0,89.1042,C,First,man,True,C,Cherbourg,yes,False\n",
    "1,1,female,24.0,3,2,263.0,S,First,woman,False,C,Southampton,yes,False\n",
    "1,1,female,30.0,0,0,93.5,S,First,woman,False,B,Southampton,yes,True\n",
    "0,3,female,,8,2,69.55,S,Third,woman,False,,Southampton,no,False\n",
    "0,1,male,,0,0,0.0,S,First,man,True,B,Southampton,no,True\n",
    "0,2,male,21.0,1,0,11.5,S,Second,man,True,,Southampton,no,False\n",
    "1,3,female,,1,0,16.1,S,Third,woman,False,,Southampton,yes,False\n",
    "1,1,male,35.0,0,0,512.3292,C,First,man,True,B,Cherbourg,yes,True\n",
    "0,3,male,38.0,0,0,8.6625,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,28.0,0,0,7.8958,S,Third,woman,False,,Southampton,no,True\n",
    "1,3,male,,0,0,7.2292,C,Third,man,True,,Cherbourg,yes,True\n",
    "1,1,female,39.0,1,1,110.8833,C,First,woman,False,C,Cherbourg,yes,False\n",
    "1,2,female,25.0,0,1,26.0,S,Second,woman,False,,Southampton,yes,False\n",
    "1,2,female,50.0,0,0,10.5,S,Second,woman,False,,Southampton,yes,True\n",
    "0,1,male,38.0,0,0,0.0,S,First,man,True,,Southampton,no,True\n",
    "0,1,male,65.0,0,1,61.9792,C,First,man,True,B,Cherbourg,no,False\n",
    "0,1,male,47.0,0,0,52.0,S,First,man,True,C,Southampton,no,True\n",
    "0,3,male,17.0,1,0,7.0542,S,Third,man,True,,Southampton,no,False\n",
    "0,3,male,,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,2,male,35.0,0,0,26.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,42.0,0,0,7.55,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,30.0,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,3,male,,0,0,8.4583,Q,Third,man,True,,Queenstown,no,True\n",
    "1,1,female,24.0,0,0,83.1583,C,First,woman,False,C,Cherbourg,yes,True\n",
    "0,3,male,20.0,0,0,7.8542,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,21.0,0,0,7.8542,S,Third,man,True,,Southampton,no,True\n",
    "1,3,female,24.0,1,0,15.85,S,Third,woman,False,,Southampton,yes,False\n",
    "0,1,male,40.0,0,0,0.0,S,First,man,True,B,Southampton,no,True\n",
    "1,3,male,4.0,1,1,11.1333,S,Third,child,False,,Southampton,yes,False\n",
    "0,3,male,44.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,27.0,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,25.0,0,0,7.05,S,Third,man,True,,Southampton,no,True\n",
    "1,2,female,13.0,0,1,19.5,S,Second,child,False,,Southampton,yes,False\n",
    "1,3,female,5.0,2,1,19.2583,C,Third,child,False,,Cherbourg,yes,False\n",
    "1,1,male,23.0,0,1,63.3583,C,First,man,True,D,Cherbourg,yes,False\n",
    "0,3,male,25.0,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "1,3,male,0.42,0,1,8.5167,C,Third,child,False,,Cherbourg,yes,False\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,19.0,0,0,8.05,S,Third,man,True,,Southampton,yes,True\n",
    "0,3,male,24.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,1,0,19.9667,S,Third,man,True,,Southampton,no,False\n",
    "0,3,male,29.0,0,0,7.775,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,1,male,52.0,1,1,79.65,S,First,man,True,E,Southampton,no,False\n",
    "0,1,male,54.0,0,1,77.2875,S,First,man,True,D,Southampton,no,False\n",
    "0,3,male,,0,0,14.5,S,Third,man,True,,Southampton,no,True\n",
    "1,1,male,,0,0,30.0,S,First,man,True,D,Southampton,yes,True\n",
    "0,3,male,26.0,0,0,7.8875,S,Third,man,True,,Southampton,no,True\n",
    "0,1,male,42.0,1,0,52.0,S,First,man,True,,Southampton,no,False\n",
    "0,3,male,41.0,0,0,7.125,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,32.0,0,0,8.05,S,Third,man,True,E,Southampton,yes,True\n",
    "0,2,male,,0,0,0.0,S,Second,man,True,,Southampton,no,True\n",
    "1,2,female,24.0,1,0,26.0,S,Second,woman,False,,Southampton,yes,False\n",
    "0,3,male,22.0,0,0,7.7958,S,Third,man,True,,Southampton,no,True\n",
    "0,2,male,25.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,22.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "1,2,female,50.0,0,0,10.5,S,Second,woman,False,,Southampton,yes,True\n",
    "0,3,male,24.0,0,0,7.4958,S,Third,man,True,,Southampton,no,True\n",
    "1,1,female,60.0,1,0,75.25,C,First,woman,False,D,Cherbourg,yes,False\n",
    "0,3,male,16.0,0,0,9.2167,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,,0,0,7.6292,Q,Third,woman,False,,Queenstown,no,True\n",
    "0,3,female,9.0,3,2,27.9,S,Third,child,False,,Southampton,no,False\n",
    "0,1,male,,0,0,26.0,S,First,man,True,A,Southampton,no,True\n",
    "0,3,female,9.0,2,2,34.375,S,Third,child,False,,Southampton,no,False\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,28.5,0,0,16.1,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\n",
    "1,2,female,32.5,0,0,13.0,S,Second,woman,False,E,Southampton,yes,True\n",
    "0,2,male,18.0,0,0,11.5,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,30.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,25.0,0,0,0.0,S,Third,man,True,,Southampton,yes,True\n",
    "0,3,female,2.0,4,2,31.275,S,Third,child,False,,Southampton,no,False\n",
    "0,3,male,29.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,29.0,0,0,7.875,S,Third,man,True,,Southampton,no,True\n",
    "------------------\n",
    "Answer between 0~1 what happened to the \"SURVIVED\" of the following people when the above data was available.\n",
    "Output should be numeric only.\n",
    "------------------\n",
    "pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "2,male,27.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "\"\"\"\n",
    "\n",
    "ctagi.run(input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Search tools.\n",
      "\u001b[32mSearch results：\n",
      "[ScoredPoint(id=1, version=0, score=0.8188539295212875, payload={'description': 'Predicts the survival of a passenger based on the given data. The input is a single string with passenger information.', 'code': 'from langchain.tools import BaseTool\\n\\nclass TitanicPredictionTool(BaseTool):\\n    name = \"TitanicPredictionTool\"\\n    description = \"Predicts the survival of a passenger based on the given data. The input is a single string with passenger information.\"\\n\\n    def _run(self, query: str) -> float:\\n        \"Use the tool.\"\\n        data = query.split(\",\")\\n        pclass, sex, age, sibsp, parch, fare, embarked, _, _, _, _, _, _, _ = data\\n\\n        # Basic logic for prediction based on the given data\\n        survival_chance = 0.5\\n\\n        if pclass == \"1\":\\n            survival_chance += 0.1\\n        elif pclass == \"3\":\\n            survival_chance -= 0.1\\n\\n        if sex == \"female\":\\n            survival_chance += 0.3\\n        else:\\n            survival_chance -= 0.3\\n\\n        if age:\\n            age = float(age)\\n            if age <= 16:\\n                survival_chance += 0.1\\n            elif age >= 60:\\n                survival_chance -= 0.1\\n\\n        if sibsp != \"0\" or parch != \"0\":\\n            survival_chance += 0.05\\n        else:\\n            survival_chance -= 0.05\\n\\n        if embarked == \"C\":\\n            survival_chance += 0.05\\n        elif embarked == \"Q\":\\n            survival_chance -= 0.05\\n\\n        # Ensure survival_chance is between 0 and 1\\n        survival_chance = max(0, min(1, survival_chance))\\n\\n        return survival_chance\\n\\n    async def _arun(self, query: str) -> float:\\n        \"Use the tool asynchronously.\"\\n        raise NotImplementedError(\"TitanicPredictionTool does not support async\")', 'tool_name': 'TitanicPredictionTool'}, vector=None)]\n",
      "\u001b[0m\n",
      "> Decide if the task is executable.\n",
      "Response：Yes.\n",
      "\u001b[32mI do not need to create a tool to run it.\n",
      "\u001b[0m\n",
      "> Execute the task.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: `I need to use the TitanicPredictionTool to predict the survival of this passenger.`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mPlease predict the following titanic passengers.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m------------------\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mpclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m3,male,32.0,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mctagi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py:55\u001b[0m, in \u001b[0;36mCTAGI.run\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m decision \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecider\u001b[39m.\u001b[39mrun(generalized_input, search_result \u001b[39m=\u001b[39m search_result)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m decision \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYes.\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecutor\u001b[39m.\u001b[39;49mrun(\u001b[39minput\u001b[39;49m, search_result \u001b[39m=\u001b[39;49m search_result)\n\u001b[1;32m     57\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m \u001b[39m#elif decision == \"No.\":\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py:38\u001b[0m, in \u001b[0;36mExecutor.run\u001b[0;34m(self, input, search_result)\u001b[0m\n\u001b[1;32m     29\u001b[0m tools \u001b[39m=\u001b[39m namespace[\u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     31\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(\n\u001b[1;32m     32\u001b[0m     tools, \n\u001b[1;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \n\u001b[1;32m     34\u001b[0m     agent\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mzero-shot-react-description\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     35\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[0;32m---> 38\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:905\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 905\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    906\u001b[0m         name_to_tool_map,\n\u001b[1;32m    907\u001b[0m         color_mapping,\n\u001b[1;32m    908\u001b[0m         inputs,\n\u001b[1;32m    909\u001b[0m         intermediate_steps,\n\u001b[1;32m    910\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    913\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m    914\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m    915\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:749\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors:\n\u001b[0;32m--> 749\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    750\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m    751\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInvalid or incomplete response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:742\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \n\u001b[1;32m    738\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 742\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    743\u001b[0m         intermediate_steps,\n\u001b[1;32m    744\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    745\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:426\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    425\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:26\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(regex, text, re\u001b[39m.\u001b[39mDOTALL)\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m action \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m action_input \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `I need to use the TitanicPredictionTool to predict the survival of this passenger.`"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "input = \"\"\"\n",
    "Please predict the following titanic passengers.\n",
    "------------------\n",
    "pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "3,male,32.0,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\n",
    "\"\"\"\n",
    "\n",
    "ctagi.run(input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
