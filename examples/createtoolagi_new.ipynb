{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateTool-AGI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/CreateTool-AGI/blob/main/examples/createtoolagi_new.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/fuyu-quant/CreateTool-AGI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "from createtoolagi import CTAGI\n",
    "\n",
    "#import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "import pkg_resources\n",
    "package_name = \"createtoolagi\"  \n",
    "pkg = pkg_resources.get_distribution(package_name)\n",
    "print(pkg.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model_name = 'gpt-4')\n",
    "gpt35_turbo = OpenAI(temperature=0, model_name = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'What is the sum of the prime numbers from 1 to 100000?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "survived,pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "0,2,male,52.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.25,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,18.0,0,0,7.775,S,Third,woman,False,,Southampton,no,True\n",
    "1,3,female,16.0,0,0,7.75,Q,Third,woman,False,,Queenstown,yes,True\n",
    "0,2,male,19.0,1,1,36.75,S,Second,man,True,,Southampton,no,False\n",
    "0,3,male,28.0,0,0,56.4958,S,Third,man,True,,Southampton,no,True\n",
    "1,1,female,53.0,2,0,51.4792,S,First,woman,False,C,Southampton,yes,False\n",
    "1,3,female,4.0,0,1,13.4167,C,Third,child,False,,Cherbourg,yes,False\n",
    "1,3,male,32.0,0,0,7.925,S,Third,man,True,,Southampton,yes,True\n",
    "0,2,male,23.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,1,female,25.0,1,2,151.55,S,First,woman,False,C,Southampton,no,False\n",
    "1,3,male,20.0,0,0,7.2292,C,Third,man,True,,Cherbourg,yes,True\n",
    "0,3,male,19.0,0,0,7.65,S,Third,man,True,F,Southampton,no,True\n",
    "0,3,male,39.0,0,0,24.15,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,32.0,0,0,56.4958,S,Third,man,True,,Southampton,yes,True\n",
    "0,2,male,39.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "1,1,female,31.0,0,2,164.8667,S,First,woman,False,C,Southampton,yes,False\n",
    "1,3,male,20.0,1,0,7.925,S,Third,man,True,,Southampton,yes,False\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,51.0,0,0,7.0542,S,Third,man,True,,Southampton,no,True\n",
    "1,3,female,18.0,0,0,9.8417,S,Third,woman,False,,Southampton,yes,True\n",
    "1,1,male,49.0,1,0,89.1042,C,First,man,True,C,Cherbourg,yes,False\n",
    "1,1,female,24.0,3,2,263.0,S,First,woman,False,C,Southampton,yes,False\n",
    "1,1,female,30.0,0,0,93.5,S,First,woman,False,B,Southampton,yes,True\n",
    "0,3,female,,8,2,69.55,S,Third,woman,False,,Southampton,no,False\n",
    "0,1,male,,0,0,0.0,S,First,man,True,B,Southampton,no,True\n",
    "0,2,male,21.0,1,0,11.5,S,Second,man,True,,Southampton,no,False\n",
    "1,3,female,,1,0,16.1,S,Third,woman,False,,Southampton,yes,False\n",
    "1,1,male,35.0,0,0,512.3292,C,First,man,True,B,Cherbourg,yes,True\n",
    "0,3,male,38.0,0,0,8.6625,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,28.0,0,0,7.8958,S,Third,woman,False,,Southampton,no,True\n",
    "1,3,male,,0,0,7.2292,C,Third,man,True,,Cherbourg,yes,True\n",
    "1,1,female,39.0,1,1,110.8833,C,First,woman,False,C,Cherbourg,yes,False\n",
    "1,2,female,25.0,0,1,26.0,S,Second,woman,False,,Southampton,yes,False\n",
    "1,2,female,50.0,0,0,10.5,S,Second,woman,False,,Southampton,yes,True\n",
    "0,1,male,38.0,0,0,0.0,S,First,man,True,,Southampton,no,True\n",
    "0,1,male,65.0,0,1,61.9792,C,First,man,True,B,Cherbourg,no,False\n",
    "0,1,male,47.0,0,0,52.0,S,First,man,True,C,Southampton,no,True\n",
    "0,3,male,17.0,1,0,7.0542,S,Third,man,True,,Southampton,no,False\n",
    "0,3,male,,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,2,male,35.0,0,0,26.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,42.0,0,0,7.55,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,30.0,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,3,male,,0,0,8.4583,Q,Third,man,True,,Queenstown,no,True\n",
    "1,1,female,24.0,0,0,83.1583,C,First,woman,False,C,Cherbourg,yes,True\n",
    "0,3,male,20.0,0,0,7.8542,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,21.0,0,0,7.8542,S,Third,man,True,,Southampton,no,True\n",
    "1,3,female,24.0,1,0,15.85,S,Third,woman,False,,Southampton,yes,False\n",
    "0,1,male,40.0,0,0,0.0,S,First,man,True,B,Southampton,no,True\n",
    "1,3,male,4.0,1,1,11.1333,S,Third,child,False,,Southampton,yes,False\n",
    "0,3,male,44.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,27.0,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,25.0,0,0,7.05,S,Third,man,True,,Southampton,no,True\n",
    "1,2,female,13.0,0,1,19.5,S,Second,child,False,,Southampton,yes,False\n",
    "1,3,female,5.0,2,1,19.2583,C,Third,child,False,,Cherbourg,yes,False\n",
    "1,1,male,23.0,0,1,63.3583,C,First,man,True,D,Cherbourg,yes,False\n",
    "0,3,male,25.0,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "1,3,male,0.42,0,1,8.5167,C,Third,child,False,,Cherbourg,yes,False\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,19.0,0,0,8.05,S,Third,man,True,,Southampton,yes,True\n",
    "0,3,male,24.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,1,0,19.9667,S,Third,man,True,,Southampton,no,False\n",
    "0,3,male,29.0,0,0,7.775,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,1,male,52.0,1,1,79.65,S,First,man,True,E,Southampton,no,False\n",
    "0,1,male,54.0,0,1,77.2875,S,First,man,True,D,Southampton,no,False\n",
    "0,3,male,,0,0,14.5,S,Third,man,True,,Southampton,no,True\n",
    "1,1,male,,0,0,30.0,S,First,man,True,D,Southampton,yes,True\n",
    "0,3,male,26.0,0,0,7.8875,S,Third,man,True,,Southampton,no,True\n",
    "0,1,male,42.0,1,0,52.0,S,First,man,True,,Southampton,no,False\n",
    "0,3,male,41.0,0,0,7.125,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,32.0,0,0,8.05,S,Third,man,True,E,Southampton,yes,True\n",
    "0,2,male,,0,0,0.0,S,Second,man,True,,Southampton,no,True\n",
    "1,2,female,24.0,1,0,26.0,S,Second,woman,False,,Southampton,yes,False\n",
    "0,3,male,22.0,0,0,7.7958,S,Third,man,True,,Southampton,no,True\n",
    "0,2,male,25.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,22.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "1,2,female,50.0,0,0,10.5,S,Second,woman,False,,Southampton,yes,True\n",
    "0,3,male,24.0,0,0,7.4958,S,Third,man,True,,Southampton,no,True\n",
    "1,1,female,60.0,1,0,75.25,C,First,woman,False,D,Cherbourg,yes,False\n",
    "0,3,male,16.0,0,0,9.2167,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,,0,0,7.6292,Q,Third,woman,False,,Queenstown,no,True\n",
    "0,3,female,9.0,3,2,27.9,S,Third,child,False,,Southampton,no,False\n",
    "0,1,male,,0,0,26.0,S,First,man,True,A,Southampton,no,True\n",
    "0,3,female,9.0,2,2,34.375,S,Third,child,False,,Southampton,no,False\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,28.5,0,0,16.1,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\n",
    "1,2,female,32.5,0,0,13.0,S,Second,woman,False,E,Southampton,yes,True\n",
    "0,2,male,18.0,0,0,11.5,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,30.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,25.0,0,0,0.0,S,Third,man,True,,Southampton,yes,True\n",
    "0,3,female,2.0,4,2,31.275,S,Third,child,False,,Southampton,no,False\n",
    "0,3,male,29.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,29.0,0,0,7.875,S,Third,man,True,,Southampton,no,True\n",
    "------------------\n",
    "Answer between 0~1 what happened to the \"SURVIVED\" of the following people when the above data was available.\n",
    "Output should be numeric only.\n",
    "------------------\n",
    "pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "2,male,27.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgpt4\u001b[49m(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt4' is not defined"
     ]
    }
   ],
   "source": [
    "gpt4(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We can use a sieve to find all the prime numbers from 1 to 100000. Then, we can add them up to find the sum.\\n\\nHere's the Python code to implement the sieve:\\n\\n```python\\ndef sieve(n):\\n    primes = [True] * (n+1)\\n    primes[0] = primes[1] = False\\n    for i in range(2, int(n**0.5)+1):\\n        if primes[i]:\\n            for j in range(i*i, n+1, i):\\n                primes[j] = False\\n    return [i for i in range(n+1) if primes[i]]\\n```\\n\\nUsing this function, we can find all the prime numbers from 1 to 100000:\\n\\n```python\\nprimes = sieve(100000)\\n```\\n\\nNow, we just need to add them up:\\n\\n```python\\nsum_of_primes = sum(primes)\\n```\\n\\nThe answer is:\\n\\n```\\nsum_of_primes = 454396537\\n```\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_turbo(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CreateTool-AGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from createtoolagi import CTAGI\n",
    "\n",
    "base_model_name = 'gpt-4'\n",
    "#base_model_name = 'gpt-3.5-turbo'\n",
    "create_model_name = 'gpt-4'\n",
    "embegging_model_name = 'text-embedding-ada-002'\n",
    "\n",
    "ctagi = CTAGI(\n",
    "    base_model_name=base_model_name, \n",
    "    create_model_name=create_model_name, \n",
    "    embegging_model_name=embegging_model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Search tools.\n",
      "\u001b[32mSearch results：\n",
      "[]\n",
      "\u001b[0m\n",
      "> Decide if the task is executable.\n",
      "Response：No.\n",
      "\u001b[32mI must create the tool before executing.\n",
      "\u001b[0m\n",
      "Try:1\n",
      "> Create a tool.\n",
      "\u001b[32mCompleted!\n",
      "Created tool name：SurvivalPredictionTool\n",
      "\u001b[0m\n",
      "Created tool code: from langchain.tools import BaseTool\n",
      "\n",
      "class SurvivalPredictionTool(BaseTool):\n",
      "    name = \"SurvivalPredictionTool\"\n",
      "    description = \"Predicts the survival of a person based on the given data. The input is a single string with the person's information.\"\n",
      "\n",
      "    def _run(self, query: str) -> float:\n",
      "        \"Use the tool.\"\n",
      "        data = query.split(\",\")\n",
      "\n",
      "        pclass, sex, age, sibsp, parch, fare, embarked, pclass_str, who, adult_male, deck, embark_town, alive, alone = data\n",
      "\n",
      "        # Basic logic for survival prediction\n",
      "        if sex == \"female\":\n",
      "            if pclass == \"1\" or pclass == \"2\":\n",
      "                return 1.0\n",
      "            elif age and float(age) <= 18:\n",
      "                return 0.75\n",
      "            else:\n",
      "                return 0.5\n",
      "        else:\n",
      "            if pclass == \"1\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.5\n",
      "                else:\n",
      "                    return 0.35\n",
      "            elif pclass == \"2\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.25\n",
      "                else:\n",
      "                    return 0.15\n",
      "            else:\n",
      "                return 0.1\n",
      "\n",
      "    async def _arun(self, query: str) -> float:\n",
      "        \"Use the tool asynchronously.\"\n",
      "        raise NotImplementedError(\"SurvivalPredictionTool does not support async\")\n",
      "> Execute the task with the tool you created.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32mError occurred: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the given person.`\n",
      "\u001b[0m\n",
      "Try:2\n",
      "> Create a tool.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py\", line 70, in run\n",
      "    self.executor.run_with_create_tool(input, tool_code = tool_code)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py\", line 62, in run_with_create_tool\n",
      "    agent.run(input)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 236, in run\n",
      "    return self(args[0], callbacks=callbacks)[self.output_keys[0]]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 905, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 749, in _take_next_step\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 742, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 426, in plan\n",
      "    return self.output_parser.parse(full_output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py\", line 26, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
      "langchain.schema.OutputParserException: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the given person.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCompleted!\n",
      "Created tool name：SurvivalPredictionTool\n",
      "\u001b[0m\n",
      "Created tool code: from langchain.tools import BaseTool\n",
      "\n",
      "class SurvivalPredictionTool(BaseTool):\n",
      "    name = \"SurvivalPredictionTool\"\n",
      "    description = \"Predicts the survival of a person based on the given data. The input is a single string with the person's information.\"\n",
      "\n",
      "    def _run(self, query: str) -> float:\n",
      "        \"Use the tool.\"\n",
      "        data = query.split(\",\")\n",
      "\n",
      "        pclass, sex, age, sibsp, parch, fare, embarked, pclass_str, who, adult_male, deck, embark_town, alive, alone = data\n",
      "\n",
      "        # Basic logic for survival prediction\n",
      "        if sex == \"female\":\n",
      "            if pclass == \"1\" or pclass == \"2\":\n",
      "                return 1.0\n",
      "            elif age and float(age) <= 18:\n",
      "                return 0.75\n",
      "            else:\n",
      "                return 0.5\n",
      "        else:\n",
      "            if pclass == \"1\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.5\n",
      "                else:\n",
      "                    return 0.35\n",
      "            elif pclass == \"2\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.25\n",
      "                else:\n",
      "                    return 0.15\n",
      "            else:\n",
      "                return 0.1\n",
      "\n",
      "    async def _arun(self, query: str) -> float:\n",
      "        \"Use the tool asynchronously.\"\n",
      "        raise NotImplementedError(\"SurvivalPredictionTool does not support async\")\n",
      "> Execute the task with the tool you created.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32mError occurred: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n",
      "\u001b[0m\n",
      "Try:3\n",
      "> Create a tool.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py\", line 70, in run\n",
      "    self.executor.run_with_create_tool(input, tool_code = tool_code)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py\", line 62, in run_with_create_tool\n",
      "    agent.run(input)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 236, in run\n",
      "    return self(args[0], callbacks=callbacks)[self.output_keys[0]]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 905, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 749, in _take_next_step\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 742, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 426, in plan\n",
      "    return self.output_parser.parse(full_output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py\", line 26, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
      "langchain.schema.OutputParserException: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCompleted!\n",
      "Created tool name：SurvivalPredictionTool\n",
      "\u001b[0m\n",
      "Created tool code: from langchain.tools import BaseTool\n",
      "\n",
      "class SurvivalPredictionTool(BaseTool):\n",
      "    name = \"SurvivalPredictionTool\"\n",
      "    description = \"Predicts the survival of a person based on the given data. The input is a single string with the person's information.\"\n",
      "\n",
      "    def _run(self, query: str) -> float:\n",
      "        \"Use the tool.\"\n",
      "        data = query.split(\",\")\n",
      "\n",
      "        pclass, sex, age, sibsp, parch, fare, embarked, pclass_str, who, adult_male, deck, embark_town, alive, alone = data\n",
      "\n",
      "        # Basic logic for survival prediction\n",
      "        if sex == \"female\":\n",
      "            if pclass == \"1\" or pclass == \"2\":\n",
      "                return 1.0\n",
      "            elif age and float(age) <= 18:\n",
      "                return 0.75\n",
      "            else:\n",
      "                return 0.5\n",
      "        else:\n",
      "            if pclass == \"1\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.5\n",
      "                else:\n",
      "                    return 0.35\n",
      "            elif pclass == \"2\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.25\n",
      "                else:\n",
      "                    return 0.15\n",
      "            else:\n",
      "                return 0.1\n",
      "\n",
      "    async def _arun(self, query: str) -> float:\n",
      "        \"Use the tool asynchronously.\"\n",
      "        raise NotImplementedError(\"SurvivalPredictionTool does not support async\")\n",
      "> Execute the task with the tool you created.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32mError occurred: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n",
      "\u001b[0m\n",
      "Try:4\n",
      "> Create a tool.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py\", line 70, in run\n",
      "    self.executor.run_with_create_tool(input, tool_code = tool_code)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py\", line 62, in run_with_create_tool\n",
      "    agent.run(input)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 236, in run\n",
      "    return self(args[0], callbacks=callbacks)[self.output_keys[0]]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 905, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 749, in _take_next_step\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 742, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 426, in plan\n",
      "    return self.output_parser.parse(full_output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py\", line 26, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
      "langchain.schema.OutputParserException: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCompleted!\n",
      "Created tool name：SurvivalPredictionTool\n",
      "\u001b[0m\n",
      "Created tool code: from langchain.tools import BaseTool\n",
      "\n",
      "class SurvivalPredictionTool(BaseTool):\n",
      "    name = \"SurvivalPredictionTool\"\n",
      "    description = \"Predicts the survival of a person based on the given data. The input is a single string with the person's information.\"\n",
      "\n",
      "    def _run(self, query: str) -> float:\n",
      "        \"Use the tool.\"\n",
      "        data = query.split(\",\")\n",
      "\n",
      "        pclass, sex, age, sibsp, parch, fare, embarked, pclass_str, who, adult_male, deck, embark_town, alive, alone = data\n",
      "\n",
      "        # Basic logic for survival prediction\n",
      "        if sex == \"female\":\n",
      "            if pclass == \"1\" or pclass == \"2\":\n",
      "                return 1.0\n",
      "            elif age and float(age) <= 18:\n",
      "                return 0.75\n",
      "            else:\n",
      "                return 0.5\n",
      "        else:\n",
      "            if pclass == \"1\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.5\n",
      "                else:\n",
      "                    return 0.35\n",
      "            elif pclass == \"2\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.25\n",
      "                else:\n",
      "                    return 0.15\n",
      "            else:\n",
      "                return 0.1\n",
      "\n",
      "    async def _arun(self, query: str) -> float:\n",
      "        \"Use the tool asynchronously.\"\n",
      "        raise NotImplementedError(\"SurvivalPredictionTool does not support async\")\n",
      "> Execute the task with the tool you created.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32mError occurred: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n",
      "\u001b[0m\n",
      "Try:5\n",
      "> Create a tool.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py\", line 70, in run\n",
      "    self.executor.run_with_create_tool(input, tool_code = tool_code)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py\", line 62, in run_with_create_tool\n",
      "    agent.run(input)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 236, in run\n",
      "    return self(args[0], callbacks=callbacks)[self.output_keys[0]]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 905, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 749, in _take_next_step\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 742, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 426, in plan\n",
      "    return self.output_parser.parse(full_output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py\", line 26, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
      "langchain.schema.OutputParserException: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCompleted!\n",
      "Created tool name：SurvivalPredictionTool\n",
      "\u001b[0m\n",
      "Created tool code: from langchain.tools import BaseTool\n",
      "\n",
      "class SurvivalPredictionTool(BaseTool):\n",
      "    name = \"SurvivalPredictionTool\"\n",
      "    description = \"Predicts the survival of a person based on the given data. The input is a single string with the person's information.\"\n",
      "\n",
      "    def _run(self, query: str) -> float:\n",
      "        \"Use the tool.\"\n",
      "        data = query.split(\",\")\n",
      "\n",
      "        pclass, sex, age, sibsp, parch, fare, embarked, pclass_str, who, adult_male, deck, embark_town, alive, alone = data\n",
      "\n",
      "        # Basic logic for survival prediction\n",
      "        if sex == \"female\":\n",
      "            if pclass == \"1\" or pclass == \"2\":\n",
      "                return 1.0\n",
      "            elif age and float(age) <= 18:\n",
      "                return 0.75\n",
      "            else:\n",
      "                return 0.5\n",
      "        else:\n",
      "            if pclass == \"1\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.5\n",
      "                else:\n",
      "                    return 0.35\n",
      "            elif pclass == \"2\":\n",
      "                if age and float(age) <= 18:\n",
      "                    return 0.25\n",
      "                else:\n",
      "                    return 0.15\n",
      "            else:\n",
      "                return 0.1\n",
      "\n",
      "    async def _arun(self, query: str) -> float:\n",
      "        \"Use the tool asynchronously.\"\n",
      "        raise NotImplementedError(\"SurvivalPredictionTool does not support async\")\n",
      "> Execute the task with the tool you created.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32mError occurred: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n",
      "\u001b[0m\n",
      "\u001b[32mReached the maximum number of tries.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py\", line 70, in run\n",
      "    self.executor.run_with_create_tool(input, tool_code = tool_code)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py\", line 62, in run_with_create_tool\n",
      "    agent.run(input)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 236, in run\n",
      "    return self(args[0], callbacks=callbacks)[self.output_keys[0]]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 905, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 749, in _take_next_step\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 742, in _take_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 426, in plan\n",
      "    return self.output_parser.parse(full_output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py\", line 26, in parse\n",
      "    raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
      "langchain.schema.OutputParserException: Could not parse LLM output: `I need to use the SurvivalPredictionTool to predict the survival of the person with the given data.`\n"
     ]
    }
   ],
   "source": [
    "input = \"\"\"\n",
    "survived,pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "0,2,male,52.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.25,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,18.0,0,0,7.775,S,Third,woman,False,,Southampton,no,True\n",
    "1,3,female,16.0,0,0,7.75,Q,Third,woman,False,,Queenstown,yes,True\n",
    "0,2,male,19.0,1,1,36.75,S,Second,man,True,,Southampton,no,False\n",
    "0,3,male,28.0,0,0,56.4958,S,Third,man,True,,Southampton,no,True\n",
    "1,1,female,53.0,2,0,51.4792,S,First,woman,False,C,Southampton,yes,False\n",
    "1,3,female,4.0,0,1,13.4167,C,Third,child,False,,Cherbourg,yes,False\n",
    "1,3,male,32.0,0,0,7.925,S,Third,man,True,,Southampton,yes,True\n",
    "0,2,male,23.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,1,female,25.0,1,2,151.55,S,First,woman,False,C,Southampton,no,False\n",
    "1,3,male,20.0,0,0,7.2292,C,Third,man,True,,Cherbourg,yes,True\n",
    "0,3,male,19.0,0,0,7.65,S,Third,man,True,F,Southampton,no,True\n",
    "0,3,male,39.0,0,0,24.15,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,32.0,0,0,56.4958,S,Third,man,True,,Southampton,yes,True\n",
    "0,2,male,39.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "1,1,female,31.0,0,2,164.8667,S,First,woman,False,C,Southampton,yes,False\n",
    "1,3,male,20.0,1,0,7.925,S,Third,man,True,,Southampton,yes,False\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,51.0,0,0,7.0542,S,Third,man,True,,Southampton,no,True\n",
    "1,3,female,18.0,0,0,9.8417,S,Third,woman,False,,Southampton,yes,True\n",
    "1,1,male,49.0,1,0,89.1042,C,First,man,True,C,Cherbourg,yes,False\n",
    "1,1,female,24.0,3,2,263.0,S,First,woman,False,C,Southampton,yes,False\n",
    "1,1,female,30.0,0,0,93.5,S,First,woman,False,B,Southampton,yes,True\n",
    "0,3,female,,8,2,69.55,S,Third,woman,False,,Southampton,no,False\n",
    "0,1,male,,0,0,0.0,S,First,man,True,B,Southampton,no,True\n",
    "0,2,male,21.0,1,0,11.5,S,Second,man,True,,Southampton,no,False\n",
    "1,3,female,,1,0,16.1,S,Third,woman,False,,Southampton,yes,False\n",
    "1,1,male,35.0,0,0,512.3292,C,First,man,True,B,Cherbourg,yes,True\n",
    "0,3,male,38.0,0,0,8.6625,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,28.0,0,0,7.8958,S,Third,woman,False,,Southampton,no,True\n",
    "1,3,male,,0,0,7.2292,C,Third,man,True,,Cherbourg,yes,True\n",
    "1,1,female,39.0,1,1,110.8833,C,First,woman,False,C,Cherbourg,yes,False\n",
    "1,2,female,25.0,0,1,26.0,S,Second,woman,False,,Southampton,yes,False\n",
    "1,2,female,50.0,0,0,10.5,S,Second,woman,False,,Southampton,yes,True\n",
    "0,1,male,38.0,0,0,0.0,S,First,man,True,,Southampton,no,True\n",
    "0,1,male,65.0,0,1,61.9792,C,First,man,True,B,Cherbourg,no,False\n",
    "0,1,male,47.0,0,0,52.0,S,First,man,True,C,Southampton,no,True\n",
    "0,3,male,17.0,1,0,7.0542,S,Third,man,True,,Southampton,no,False\n",
    "0,3,male,,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,2,male,35.0,0,0,26.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,42.0,0,0,7.55,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,30.0,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,3,male,,0,0,8.4583,Q,Third,man,True,,Queenstown,no,True\n",
    "1,1,female,24.0,0,0,83.1583,C,First,woman,False,C,Cherbourg,yes,True\n",
    "0,3,male,20.0,0,0,7.8542,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,21.0,0,0,7.8542,S,Third,man,True,,Southampton,no,True\n",
    "1,3,female,24.0,1,0,15.85,S,Third,woman,False,,Southampton,yes,False\n",
    "0,1,male,40.0,0,0,0.0,S,First,man,True,B,Southampton,no,True\n",
    "1,3,male,4.0,1,1,11.1333,S,Third,child,False,,Southampton,yes,False\n",
    "0,3,male,44.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,27.0,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,25.0,0,0,7.05,S,Third,man,True,,Southampton,no,True\n",
    "1,2,female,13.0,0,1,19.5,S,Second,child,False,,Southampton,yes,False\n",
    "1,3,female,5.0,2,1,19.2583,C,Third,child,False,,Cherbourg,yes,False\n",
    "1,1,male,23.0,0,1,63.3583,C,First,man,True,D,Cherbourg,yes,False\n",
    "0,3,male,25.0,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "1,3,male,0.42,0,1,8.5167,C,Third,child,False,,Cherbourg,yes,False\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,19.0,0,0,8.05,S,Third,man,True,,Southampton,yes,True\n",
    "0,3,male,24.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,1,0,19.9667,S,Third,man,True,,Southampton,no,False\n",
    "0,3,male,29.0,0,0,7.775,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.225,C,Third,man,True,,Cherbourg,no,True\n",
    "0,1,male,52.0,1,1,79.65,S,First,man,True,E,Southampton,no,False\n",
    "0,1,male,54.0,0,1,77.2875,S,First,man,True,D,Southampton,no,False\n",
    "0,3,male,,0,0,14.5,S,Third,man,True,,Southampton,no,True\n",
    "1,1,male,,0,0,30.0,S,First,man,True,D,Southampton,yes,True\n",
    "0,3,male,26.0,0,0,7.8875,S,Third,man,True,,Southampton,no,True\n",
    "0,1,male,42.0,1,0,52.0,S,First,man,True,,Southampton,no,False\n",
    "0,3,male,41.0,0,0,7.125,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,32.0,0,0,8.05,S,Third,man,True,E,Southampton,yes,True\n",
    "0,2,male,,0,0,0.0,S,Second,man,True,,Southampton,no,True\n",
    "1,2,female,24.0,1,0,26.0,S,Second,woman,False,,Southampton,yes,False\n",
    "0,3,male,22.0,0,0,7.7958,S,Third,man,True,,Southampton,no,True\n",
    "0,2,male,25.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,22.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "1,2,female,50.0,0,0,10.5,S,Second,woman,False,,Southampton,yes,True\n",
    "0,3,male,24.0,0,0,7.4958,S,Third,man,True,,Southampton,no,True\n",
    "1,1,female,60.0,1,0,75.25,C,First,woman,False,D,Cherbourg,yes,False\n",
    "0,3,male,16.0,0,0,9.2167,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,female,,0,0,7.6292,Q,Third,woman,False,,Queenstown,no,True\n",
    "0,3,female,9.0,3,2,27.9,S,Third,child,False,,Southampton,no,False\n",
    "0,1,male,,0,0,26.0,S,First,man,True,A,Southampton,no,True\n",
    "0,3,female,9.0,2,2,34.375,S,Third,child,False,,Southampton,no,False\n",
    "0,3,male,,0,0,7.8958,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,28.5,0,0,16.1,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\n",
    "1,2,female,32.5,0,0,13.0,S,Second,woman,False,E,Southampton,yes,True\n",
    "0,2,male,18.0,0,0,11.5,S,Second,man,True,,Southampton,no,True\n",
    "0,3,male,30.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "1,3,male,25.0,0,0,0.0,S,Third,man,True,,Southampton,yes,True\n",
    "0,3,female,2.0,4,2,31.275,S,Third,child,False,,Southampton,no,False\n",
    "0,3,male,29.0,0,0,8.05,S,Third,man,True,,Southampton,no,True\n",
    "0,3,male,29.0,0,0,7.875,S,Third,man,True,,Southampton,no,True\n",
    "------------------\n",
    "Answer between 0~1 what happened to the \"SURVIVED\" of the following people when the above data was available.\n",
    "Output should be numeric only.\n",
    "------------------\n",
    "pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "2,male,27.0,0,0,13.0,S,Second,man,True,,Southampton,no,True\n",
    "\"\"\"\n",
    "\n",
    "ctagi.run(input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Search tools.\n",
      "\u001b[32mSearch results：\n",
      "[ScoredPoint(id=1, version=0, score=0.8188539295212875, payload={'description': 'Predicts the survival of a passenger based on the given data. The input is a single string with passenger information.', 'code': 'from langchain.tools import BaseTool\\n\\nclass TitanicPredictionTool(BaseTool):\\n    name = \"TitanicPredictionTool\"\\n    description = \"Predicts the survival of a passenger based on the given data. The input is a single string with passenger information.\"\\n\\n    def _run(self, query: str) -> float:\\n        \"Use the tool.\"\\n        data = query.split(\",\")\\n        pclass, sex, age, sibsp, parch, fare, embarked, _, _, _, _, _, _, _ = data\\n\\n        # Basic logic for prediction based on the given data\\n        survival_chance = 0.5\\n\\n        if pclass == \"1\":\\n            survival_chance += 0.1\\n        elif pclass == \"3\":\\n            survival_chance -= 0.1\\n\\n        if sex == \"female\":\\n            survival_chance += 0.3\\n        else:\\n            survival_chance -= 0.3\\n\\n        if age:\\n            age = float(age)\\n            if age <= 16:\\n                survival_chance += 0.1\\n            elif age >= 60:\\n                survival_chance -= 0.1\\n\\n        if sibsp != \"0\" or parch != \"0\":\\n            survival_chance += 0.05\\n        else:\\n            survival_chance -= 0.05\\n\\n        if embarked == \"C\":\\n            survival_chance += 0.05\\n        elif embarked == \"Q\":\\n            survival_chance -= 0.05\\n\\n        # Ensure survival_chance is between 0 and 1\\n        survival_chance = max(0, min(1, survival_chance))\\n\\n        return survival_chance\\n\\n    async def _arun(self, query: str) -> float:\\n        \"Use the tool asynchronously.\"\\n        raise NotImplementedError(\"TitanicPredictionTool does not support async\")', 'tool_name': 'TitanicPredictionTool'}, vector=None)]\n",
      "\u001b[0m\n",
      "> Decide if the task is executable.\n",
      "Response：Yes.\n",
      "\u001b[32mI do not need to create a tool to run it.\n",
      "\u001b[0m\n",
      "> Execute the task.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: `I need to use the TitanicPredictionTool to predict the survival of this passenger.`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mPlease predict the following titanic passengers.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m------------------\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mpclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m3,male,32.0,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mctagi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/createtoolagi/ctagi.py:55\u001b[0m, in \u001b[0;36mCTAGI.run\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m decision \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecider\u001b[39m.\u001b[39mrun(generalized_input, search_result \u001b[39m=\u001b[39m search_result)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m decision \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYes.\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecutor\u001b[39m.\u001b[39;49mrun(\u001b[39minput\u001b[39;49m, search_result \u001b[39m=\u001b[39;49m search_result)\n\u001b[1;32m     57\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m \u001b[39m#elif decision == \"No.\":\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/createtoolagi/executor.py:38\u001b[0m, in \u001b[0;36mExecutor.run\u001b[0;34m(self, input, search_result)\u001b[0m\n\u001b[1;32m     29\u001b[0m tools \u001b[39m=\u001b[39m namespace[\u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     31\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(\n\u001b[1;32m     32\u001b[0m     tools, \n\u001b[1;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \n\u001b[1;32m     34\u001b[0m     agent\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mzero-shot-react-description\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     35\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[0;32m---> 38\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:905\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 905\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    906\u001b[0m         name_to_tool_map,\n\u001b[1;32m    907\u001b[0m         color_mapping,\n\u001b[1;32m    908\u001b[0m         inputs,\n\u001b[1;32m    909\u001b[0m         intermediate_steps,\n\u001b[1;32m    910\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    913\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m    914\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m    915\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:749\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors:\n\u001b[0;32m--> 749\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    750\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m    751\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInvalid or incomplete response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:742\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \n\u001b[1;32m    738\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 742\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    743\u001b[0m         intermediate_steps,\n\u001b[1;32m    744\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    745\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:426\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    425\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:26\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(regex, text, re\u001b[39m.\u001b[39mDOTALL)\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m action \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m action_input \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `I need to use the TitanicPredictionTool to predict the survival of this passenger.`"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "input = \"\"\"\n",
    "Please predict the following titanic passengers.\n",
    "------------------\n",
    "pclass,sex,age,sibsp,parch,fare,embarked,class,who,adult_male,deck,embark_town,alive,alone\n",
    "3,male,32.0,0,0,7.75,Q,Third,man,True,,Queenstown,no,True\n",
    "\"\"\"\n",
    "\n",
    "ctagi.run(input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generalize the input task.\n",
      "\u001b[32mGeneralized Task：What is the sum of the prime numbers from A to B?\n",
      "\u001b[0m\n",
      "> Search tools.\n",
      "\u001b[32mSearch results：\n",
      "[]\n",
      "\u001b[0m\n",
      "> Decide if the task is executable.\n",
      "Response：Sorry, I cannot answer this question without additional information. Please provide the values of A and B.\n",
      "\u001b[32mI must create the tool before executing.\n",
      "\u001b[0m\n",
      "Try:1\n",
      "> Create a tool.\n",
      "\u001b[32mCompleted!\n",
      "Created tool name：PrimeSumTool\n",
      "\u001b[0m\n",
      "> Execute the task with the tool you created.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the PrimeSumTool to calculate the sum of prime numbers between 1 and 100000.\n",
      "Action: PrimeSumTool\n",
      "Action Input: 1,100000\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m454396537\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThat's the sum of prime numbers between 1 and 100000.\n",
      "Final Answer: 454396537.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "> Save a tool.\n"
     ]
    }
   ],
   "source": [
    "input = 'What is the sum of the prime numbers from 1 to 100000?'\n",
    "\n",
    "ctagi.run(input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generalize the input task.\n",
      "\u001b[32mGeneralized Task：What is the sum of the prime numbers from A to B?\n",
      "\u001b[0m\n",
      "> Search tools.\n",
      "\u001b[32mSearch results：\n",
      "[ScoredPoint(id=1, version=0, score=0.8855925166803104, payload={'description': \"used for calculating the sum of prime numbers from A to B. The input is two numbers separated by a comma. For example, if you want to find the sum of prime numbers between 1 and 10, the input is '1,10'.\", 'code': 'from langchain.tools import BaseTool\\n\\nclass PrimeSumTool(BaseTool):\\n    name = \"PrimeSumTool\"\\n    description = \"used for calculating the sum of prime numbers from A to B. The input is two numbers separated by a comma. For example, if you want to find the sum of prime numbers between 1 and 10, the input is \\'1,10\\'.\"\\n\\n    def is_prime(self, num: int) -> bool:\\n        if num < 2:\\n            return False\\n        for i in range(2, num):\\n            if num % i == 0:\\n                return False\\n        return True\\n\\n    def _run(self, query: str) -> str:\\n        \"Use the tool.\"\\n        a, b = query.split(\",\")\\n        a, b = int(a), int(b)\\n        prime_sum = sum([i for i in range(a, b + 1) if self.is_prime(i)])\\n        result = prime_sum\\n\\n        return result\\n\\n    async def _arun(self, query: str) -> str:\\n        \"Use the tool asynchronously.\"\\n        raise NotImplementedError(\"PrimeSumTool does not support async\")', 'tool_name': 'PrimeSumTool'}, vector=None)]\n",
      "\u001b[0m\n",
      "> Decide if the task is executable.\n",
      "Response：Yes.\n",
      "\u001b[32mI do not need to create a tool to run it.\n",
      "\u001b[0m\n",
      "> Execute the task.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the PrimeSumTool to calculate the sum of prime numbers between 1 and 100000.\n",
      "Action: PrimeSumTool\n",
      "Action Input: 1,100000\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m454396537\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThat's the sum of prime numbers between 1 and 100000.\n",
      "Final Answer: 454396537\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ctagi.run(input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
